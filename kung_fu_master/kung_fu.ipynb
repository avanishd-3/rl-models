{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpx0ZfkVA7_D"
      },
      "source": [
        "# Kung Fu Master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwtJYbZB-sx"
      },
      "source": [
        "Traning A2C AI to solve [kung fu master](https://ale.farama.org/environments/kung_fu_master/) from [Gymnasium](https://gymnasium.farama.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oit0CcXdBBdS"
      },
      "source": [
        "## Intalling packages and importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRU22eeByqt"
      },
      "source": [
        "### Installing NumPy and PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IVlP4lYB7wv"
      },
      "outputs": [],
      "source": [
        "%pip install numpy\n",
        "%pip install torch\n",
        "%pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8bunpN1BGPW"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIk8QauYBIFx",
        "outputId": "b13539eb-e3b8-4579-8f91-70be6a7a00e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale-py) (1.26.4)\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379496 sha256=b1258d12315d7d16c97a8b984ec746c8282943058c07e1b5b038bb7e9908a30d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "%pip install gymnasium\n",
        "%pip install ale-py\n",
        "%pip install swig # Necessary to build the wheel for box2d-py\n",
        "%pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvBDak4PBPVm"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL8gHM8LBS6p",
        "outputId": "6a2cb512-ff3f-40eb-b2ab-c143593c8e48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Pytorch stuff\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # Neural network library\n",
        "import torch.optim as optim # Optimizer to train AI\n",
        "import torch.nn.functional as F # Activation & loss function\n",
        "import torch.multiprocessing as mp # Multiprocessing for parallel training\n",
        "import torch.distributions as distributions # Distributions for action selection\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gymnasium stuff\n",
        "import gymnasium as gym\n",
        "import ale_py # For Atari games\n",
        "from gymnasium import ObservationWrapper\n",
        "from gymnasium.spaces import Box # For environment\n",
        "from gymnasium.vector import SyncVectorEnv # Execute multiple environments in parallel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0zU9DmTB7Bw"
      },
      "source": [
        "## Building AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvYHltR6EP9P"
      },
      "source": [
        "### Neural Net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Om190UxbCVM2"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    \"\"\"\n",
        "    A2C Network with dynamically computed feature size\n",
        "\n",
        "    paramters:\n",
        "    action_size: number of actions\n",
        "    input_shape: shape of input frames (default to 4 stacked frames that are 42 x 42)\n",
        "    \"\"\"\n",
        "    def __init__(self, action_size, input_shape=(4, 42, 42)):\n",
        "        super(Network, self).__init__()\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Going to have stack of 4 frames\n",
        "        # 32 output channels to be cheap\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(3,3), stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dynamically compute feature size\n",
        "        self.feature_size = self._compute_feature_size(input_shape)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=self.feature_size, out_features=128)\n",
        "        self.fc2_action_values = nn.Linear(in_features=128, out_features=action_size) # Q values for each action\n",
        "        self.fc2_state_values = nn.Linear(in_features=128, out_features=1) # Estimate of value of current state\n",
        "\n",
        "    def _compute_feature_size(self, input_shape):\n",
        "        \"\"\"\n",
        "        Computes feature size\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          x = torch.zeros(1, *input_shape)  # Batch size of 1 with input shape\n",
        "          x = self.conv1(x)\n",
        "          x = self.conv2(x)\n",
        "          x = self.conv3(x)\n",
        "\n",
        "          return x.numel()  # Total number of elements\n",
        "\n",
        "    def forward(self, state):\n",
        "        # State here is input frames\n",
        "        x = F.relu(self.conv1(state))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = self.flatten(x) # No activation needed before flattening\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        action_values = self.fc2_action_values(x)\n",
        "        state_value = self.fc2_state_values(x).squeeze(-1) # Remove last dimension\n",
        "\n",
        "        return action_values, state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO-0WutWHxSt"
      },
      "source": [
        "## Training AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVppFmoNIApl"
      },
      "source": [
        "### Pre-process frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLOSV7OKH4RH",
        "outputId": "d2b1ac6b-5426-49c2-d80c-33561fa7c2f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class PreProcessing(ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Combines frames received to be stacks of 4 grayscale images\n",
        "\n",
        "    Paramters:\n",
        "    env: environment to wrap\n",
        "    height: height of image\n",
        "    width: width of image\n",
        "    crop: function to crop images (default is none)\n",
        "    dim_order: order of dimensions (default is (C, H, W) by PyTorch) -> tensorflow does (H, C, W)\n",
        "    color: use RGB or grasycale (default is grayscale)\n",
        "    n_frames: number of frames to combine\n",
        "    \"\"\"\n",
        "    def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
        "      super(PreProcessing, self).__init__(env)\n",
        "      self.img_size = (height, width)\n",
        "      self.crop = crop\n",
        "      self.dim_order = dim_order\n",
        "      self.color = color\n",
        "      self.frame_stack = n_frames\n",
        "      n_channels = 3 * n_frames if color else n_frames\n",
        "      obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
        "      self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "      self.frames = np.zeros(obs_shape, dtype = np.float32) # Store frames here\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "      \"\"\"\n",
        "      Reset environment to initial state and return stacked frames + environment information\n",
        "      \"\"\"\n",
        "      self.frames = np.zeros_like(self.frames) # Clear previous frames\n",
        "      obs, info = self.env.reset(**kwargs) # Reset environment\n",
        "      self.update_buffer(obs) # Pre-process first frame\n",
        "      return self.frames, info\n",
        "\n",
        "    def observation(self, img):\n",
        "      img = self.crop(img) # Apply cropping\n",
        "      img = cv2.resize(img, self.img_size) # Resize to proper dimensions\n",
        "\n",
        "      if not self.color: # Convert to grayscale\n",
        "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "          img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      img = img.astype('float32') / 255 # Map pixels to [0, 1] to improve neural net performance\n",
        "\n",
        "      # Frame stacking -> remove oldest frame + shift every frame forward when new frame received\n",
        "      # New frame is added at the end\n",
        "      if self.color:\n",
        "        self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
        "      else:\n",
        "        self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
        "\n",
        "      # Add frames to buffer\n",
        "      if self.color:\n",
        "        self.frames[-3:] = img # Replace last 3 channels for RGB\n",
        "      else:\n",
        "        self.frames[-1] = img # Replace last channel for grayscale\n",
        "\n",
        "      return self.frames\n",
        "\n",
        "    def update_buffer(self, obs):\n",
        "      \"\"\"\n",
        "      Pre-process and store frames\n",
        "      \"\"\"\n",
        "      self.frames = self.observation(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiy9U1XisSo4"
      },
      "source": [
        "### Set up Kung Fu Master environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-EPvOkH8wQU",
        "outputId": "fd9ca200-2763-4b8f-a442-cc0804a2c873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State shape: (4, 42, 42)\n",
            "Number actions: 14\n"
          ]
        }
      ],
      "source": [
        "def make_env():\n",
        "  def _init():\n",
        "    env = gym.make(\"KungFuMasterDeterministic-v4\", render_mode='rgb_array')\n",
        "    env = PreProcessing(env, height=42, width=42)  # Apply preprocessing correctly\n",
        "    return env\n",
        "  return _init  # Return the function, not an instance\n",
        "\n",
        "env = make_env().__call__()\n",
        "\n",
        "kung_fu_state_shape = env.observation_space.shape\n",
        "kung_fu_num_actions = env.action_space.n\n",
        "print(\"State shape:\", kung_fu_state_shape)\n",
        "print(\"Number actions:\", kung_fu_num_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdlsp4DJ_8Y"
      },
      "source": [
        "### Initialize hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BlzrduKhJlr0"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "discount_factor = 0.99 # Optimal discount factor\n",
        "number_environments = 10\n",
        "num_episodes = 3000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c-mCCBKOCHu"
      },
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm9KQXdEOFMt",
        "outputId": "1923e0e4-7cee-4fb2-f9fc-d762f4f953bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class Agent():\n",
        "  \"\"\"\n",
        "  Use A2C to train an agent to play Kung Fu Master\n",
        "\n",
        "  Use reward normalization via moving average to better adapt to latest rewards\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, action_size, alpha = 0.01) -> None:\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration if possible\n",
        "\n",
        "    self.action_size = action_size\n",
        "\n",
        "    self.network = Network(action_size).to(self.device) # Neural network\n",
        "    self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate) # Optimizer\n",
        "\n",
        "    # Moving average parameters for reward normalization\n",
        "    self.running_mean = 0\n",
        "    self.running_std = 1\n",
        "    self.alpha = alpha # Controls the rate of updates -> higher = more recent rewards influence mean & std more\n",
        "\n",
        "  def normalize_reward(self, reward):\n",
        "    \"\"\"\n",
        "    Normalize reward using an exponential moving average\n",
        "    \"\"\"\n",
        "\n",
        "    self.running_mean = self.alpha * reward + (1 - self.alpha) * self.running_mean\n",
        "    self.running_std = self.alpha * (reward - self.running_mean) ** 2 + (1 - self.alpha) * self.running_std\n",
        "\n",
        "    return (reward - self.running_mean) / (np.sqrt(self.running_std) + 1e-8)  # Avoid divide-by-zero\n",
        "\n",
        "  def act(self, state, epsilon=0):\n",
        "    \"\"\"\n",
        "    Agent takes actions based on given states in environment using softmax action selection policy.\n",
        "\n",
        "    Returns actions for each state in the batch\n",
        "\n",
        "    A2C is faster than deep Q learning, so can use softmax\n",
        "    \"\"\"\n",
        "\n",
        "    if state.ndim == 3: # Make sure state is in a batch\n",
        "      state = [state]\n",
        "\n",
        "    # Convert state to float32 before moving to device\n",
        "    state = torch.tensor(state, dtype=torch.float32).to(self.device)  # Convert state to tensor and add batch\n",
        "\n",
        "    with torch.no_grad(): # Disable gradients during inference\n",
        "      action_values, _ = self.network(state) # Get action values (automatically calls forward method)\n",
        "      softmax_policy = F.softmax(action_values, dim = -1)\n",
        "\n",
        "    return torch.multinomial(softmax_policy, 1).squeeze(dim=-1).cpu().numpy() # Use softmax to select action (1 sample per batch entry)\n",
        "\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    \"\"\"\n",
        "    Implement A2C formulas to make a move with normalized rewards\n",
        "    \"\"\"\n",
        "    batch_size = state.shape[0] # First dimension of state tensor is batch size\n",
        "\n",
        "    # Convert to float32 before moving to device\n",
        "    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
        "    reward = torch.tensor([self.normalize_reward(r) for r in reward], dtype=torch.float32, device=self.device)\n",
        "    done = torch.tensor(done, dtype=torch.bool, device=self.device).to(torch.float32)\n",
        "\n",
        "    action_values, state_values = self.network(state)\n",
        "    _, next_state_values = self.network(next_state)\n",
        "\n",
        "    target_state_value = reward + (1 - done) * discount_factor * next_state_values # Bellman equation\n",
        "\n",
        "    # Add A2C algorithm parts\n",
        "    advantage = target_state_value - state_values\n",
        "\n",
        "    action_distribution = F.softmax(action_values, dim = -1)\n",
        "    log_action_distribution = F.log_softmax(action_values, dim = -1)\n",
        "\n",
        "    entropy = -torch.sum(action_distribution * log_action_distribution, dim = -1) # Sum over last dimension\n",
        "\n",
        "    selected_action_log_probs = log_action_distribution[torch.arange(batch_size), action]\n",
        "\n",
        "\n",
        "    # Detach b/c don't need advantage gradients going into critic network\n",
        "    # Including entropy allows for some exploration\n",
        "    actor_loss = -(selected_action_log_probs * advantage.detach()).mean() - 0.01 * entropy.mean()\n",
        "\n",
        "\n",
        "    critic_loss = F.mse_loss(state_values, target_state_value.detach()) # Prevent target gradients from impacting calculation\n",
        "\n",
        "    total_loss = actor_loss + critic_loss\n",
        "\n",
        "    # Backpropagate results\n",
        "    self.optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbYepYcmY7OF"
      },
      "source": [
        "### Train an Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zZJ7qOczY9AH"
      },
      "outputs": [],
      "source": [
        "# Initialze an agent\n",
        "agent = Agent(action_size=kung_fu_num_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R8VL9xD6ZnPp"
      },
      "outputs": [],
      "source": [
        "# Single episode evaluation\n",
        "\n",
        "def evaluate_agent(agent, env, num_episodes = 1):\n",
        "  episode_rewards: list = [] # Rewards for each episode this is done in\n",
        "\n",
        "  for _ in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "      action = agent.act(state)\n",
        "      next_state, reward, done, env_info, _ = env.step(action[0])\n",
        "      total_reward += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "  return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLlOTOOmHU7J"
      },
      "outputs": [],
      "source": [
        "# Sychronous multiple environment evaluation\n",
        "\n",
        "class MultipleEnv:\n",
        "  def __init__(self, num_envs = 10):\n",
        "    self.envs = SyncVectorEnv([make_env() for _ in range(num_envs)])  # So environments can be executed in parallel\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset all environments simultaneously and return batched states\n",
        "    \"\"\"\n",
        "    return self.envs.reset()\n",
        "\n",
        "  def step(self, actions):\n",
        "    \"\"\"\"\n",
        "    Do action in all environments in parallel and return batched results\n",
        "    \"\"\"\n",
        "    next_states, rewards, dones, envs_info, _ = self.envs.step(actions)\n",
        "\n",
        "    # Reset environment if it has finished\n",
        "    if dones.any():\n",
        "      reset_states = self.envs.reset()\n",
        "      reset_indices = np.where(dones)[0].astype(int)\n",
        "      next_states[reset_indices] = reset_states[0][reset_indices]\n",
        "\n",
        "    return next_states, rewards, dones, envs_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otF3obiYa-ha",
        "outputId": "dea60a7b-9232-4c74-fd19-5abd1a6ea300"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/3001 [00:02<24:46,  2.02it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average agent reward: 400.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▎      | 1006/3001 [00:32<07:44,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average agent reward: 700.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2006/3001 [01:02<02:49,  5.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average agent reward: 1100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3001/3001 [01:34<00:00, 31.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average agent reward: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Final training loop\n",
        "\n",
        "import tqdm # Adds progress bar\n",
        "\n",
        "envs = MultipleEnv(number_environments)\n",
        "batch, _ = envs.reset()\n",
        "\n",
        "# Use tqdm to iterate\n",
        "with tqdm.trange(0, num_episodes + 1) as progress_bar:\n",
        "  for episode in progress_bar:\n",
        "    actions = agent.act(batch)\n",
        "    next_batch_state, rewards, dones, _ = envs.step(actions)\n",
        "\n",
        "    agent.step(batch, actions, rewards, next_batch_state, dones)\n",
        "\n",
        "    batch = next_batch_state\n",
        "\n",
        "    if (episode) % 1000 == 0: # Print average score every 1000 iterations\n",
        "      print(f\"Average agent reward: {np.mean(evaluate_agent(agent, env))}\") # Rewards are normalized, so this is not the actual reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arP7nNDooonT"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2kgCLeIpZ3x"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xicp0-kWpbQJ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6KgeB-MLRnX"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "aRBuWsBio9kA",
        "outputId": "d5c9b874-0bd0-4e88-e92f-cac757d7778e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_video_of_model(agent, video_name='video.mp4'):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "      frame = env.render()\n",
        "      frames.append(frame)\n",
        "      action = agent.act(state)\n",
        "      state, reward, done, _, _ = env.step(action[0])\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(video_name, frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, \"kung_fu_master.mp4\")\n",
        "\n",
        "def show_video():\n",
        "    # Show video in notebook\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
