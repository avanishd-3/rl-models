{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pac Man"
      ],
      "metadata": {
        "id": "Wpx0ZfkVA7_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traning  double dueling deep convolutional Q-learning AI to solve [Pac Man](https://ale.farama.org/environments/pacman/) from [Gymnasium](https://gymnasium.farama.org/)"
      ],
      "metadata": {
        "id": "JvwtJYbZB-sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intalling packages and importing libraries"
      ],
      "metadata": {
        "id": "Oit0CcXdBBdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing NumPy and PyTorch"
      ],
      "metadata": {
        "id": "8iuaM8U7CEPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy\n",
        "%pip install torch\n",
        "%pip install torchvision"
      ],
      "metadata": {
        "id": "q5uuTgtqCIjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "K8bunpN1BGPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gymnasium\n",
        "%pip install ale-py\n",
        "%pip install swig # Necessary to build the wheel for box2d-py\n",
        "%pip install gymnasium[box2d] # Contains lunar lander environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIk8QauYBIFx",
        "outputId": "9a574820-b9aa-4137-e3ea-6cfa975999e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "Successfully installed ale-py-0.10.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376425 sha256=e07e73d7f46442120ce8a266eed39159f4254f9b75e26c3e264ab202a5f0062a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "NvBDak4PBPVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Pytorch stuff\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # Neural network library\n",
        "import torch.optim as optim # Optimizer to train AI\n",
        "import torch.nn.functional as F # Activation function\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "metadata": {
        "id": "oL8gHM8LBS6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building AI"
      ],
      "metadata": {
        "id": "G0zU9DmTB7Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Net Architecture"
      ],
      "metadata": {
        "id": "PvYHltR6EP9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DuelingDCQN(nn.Module):\n",
        "  \"\"\"\n",
        "    Dueling Deep Convolutional Q-Network\n",
        "\n",
        "    Dueling separates the value of the state from the value of the actions possible in that state\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, action_size) -> None:\n",
        "    super(DuelingDCQN, self).__init__()\n",
        "\n",
        "    self.feature = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "      )\n",
        "\n",
        "    # Value stream (outputs a single scalar)\n",
        "    self.value_stream = nn.Sequential(\n",
        "        nn.Linear(10 * 10 * 128, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 1)  # Single value\n",
        "    )\n",
        "\n",
        "    # Advantage stream (outputs advantage for each action)\n",
        "    self.advantage_stream = nn.Sequential(\n",
        "        nn.Linear(10 * 10 * 128, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, action_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, state): # PyTorch needs this to be called forward to work\n",
        "\n",
        "    features = self.feature(state).view(state.size(0), -1)  # Flatten\n",
        "    value = self.value_stream(features)\n",
        "    advantage = self.advantage_stream(features)\n",
        "    # Combine value and advantage streams\n",
        "    return value + (advantage - advantage.mean(dim=1, keepdim=True))"
      ],
      "metadata": {
        "id": "Om190UxbCVM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training AI"
      ],
      "metadata": {
        "id": "SO-0WutWHxSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Pac Man environment"
      ],
      "metadata": {
        "id": "AVppFmoNIApl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "\n",
        "env = gym.make(\"MsPacmanDeterministic-v4\", full_action_space=False) # Deterministic and no full action space = less computationally expensive\n",
        "\n",
        "pac_man_state_shape = env.observation_space.shape\n",
        "pac_man_action_size = env.action_space.n\n",
        "\n",
        "# Check that environment is set up correctly\n",
        "assert(pac_man_state_shape == (210, 160, 3))\n",
        "assert(pac_man_action_size == 9)"
      ],
      "metadata": {
        "id": "tLOSV7OKH4RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize hyperparameters"
      ],
      "metadata": {
        "id": "FQdlsp4DJ_8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4 # Good for deep Q learning\n",
        "mini_batch_size = 64 # Better for Pac-Man\n",
        "discount_factor = 0.99 # Optimal discount factor\n",
        "\n",
        "# Tau removed b/c soft update doesn't improve Pac-Man results"
      ],
      "metadata": {
        "id": "BlzrduKhJlr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing frames"
      ],
      "metadata": {
        "id": "-ayxD3EJLF8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert images to PyTorch tensors\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def preprocess_frame(frame: np.ndarray):\n",
        "  \"\"\"\n",
        "  Preprocess frames so input images can be converted to PyTorch tensors\n",
        "  \"\"\"\n",
        "  frame = Image.fromarray(frame) # Convert input frame to PIL image\n",
        "\n",
        "  # Make sure image is a square\n",
        "  transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
        "\n",
        "  # Function to convert PIL image to PyTorch tensor and also normalize frames\n",
        "  frame_tensor = transform(frame) # Convert frame image to tensor\n",
        "  return frame_tensor.unsqueeze(0) # Add batch dimension"
      ],
      "metadata": {
        "id": "D9q8yXHbLHe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double Deep Q Network"
      ],
      "metadata": {
        "id": "_c-mCCBKOCHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  \"\"\"\n",
        "  Use double deep convolutional Q-learning with gradient clipping to reduce overestimation bias\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, action_size) -> None:\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration if possible\n",
        "\n",
        "    self.action_size = action_size\n",
        "\n",
        "    # Q learning\n",
        "    self.q_network = DuelingDCQN(action_size).to(self.device) # Local Q network\n",
        "    self.target_network = DuelingDCQN(action_size).to(self.device) # Target Q network\n",
        "\n",
        "    self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate) # Optimizer for Q network\n",
        "\n",
        "    self.memory = deque(maxlen=10000) # Replay memory\n",
        "\n",
        "    self.time_step = 0 # Time step for updating target network\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    state = preprocess_frame(state)\n",
        "    next_state = preprocess_frame(next_state)\n",
        "\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # Learning\n",
        "    if len(self.memory) > mini_batch_size:\n",
        "        experiences = random.sample(self.memory, k = mini_batch_size)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "\n",
        "  def act(self, state, epsilon=0):\n",
        "    \"\"\"\n",
        "    Select action based on given state in environment using epsilon-greedy action selection policy.\n",
        "\n",
        "    Epsilon-greedy is standard in Deep Q learning over softmax. It is also simpler and less computationally expensive.\n",
        "    \"\"\"\n",
        "\n",
        "    state = preprocess_frame(state).to(self.device)\n",
        "\n",
        "    self.q_network.eval()\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation (make sure in inference mode)\n",
        "      action_values = self.q_network(state)\n",
        "    self.q_network.train()\n",
        "\n",
        "    # Use epsilon greedy action-selection policy\n",
        "\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    \"\"\"\n",
        "    Update Q-values based on sampled experiences\n",
        "    \"\"\"\n",
        "\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "    # Convert elements of experience to PyTorch tensors and move them to device\n",
        "\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device).squeeze(1)\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([next_states])).float().to(self.device).squeeze(1)\n",
        "\n",
        "    # Convert Boolean data to float tensor\n",
        "    dones = torch.from_numpy(np.vstack(dones)).float().to(self.device) # Last elements in experiences\n",
        "\n",
        "    # Get best actions from the local Q-network\n",
        "    # Detach = no tracking tensor gradient during backwards propagation\n",
        "    # Unsqueeze = add batch info at index 1\n",
        "    next_actions = self.q_network(next_states).detach().argmax(1).unsqueeze(1)\n",
        "\n",
        "    # Get corresponding Q-values from the target Q-network\n",
        "    next_q_targets = self.target_network(next_states).gather(1, next_actions)\n",
        "\n",
        "    # Compute target Q-values\n",
        "    target_q_values = rewards + (gamma * next_q_targets * (1 - dones))\n",
        "\n",
        "    # Compute current Q-values\n",
        "    predicted_q_values = self.q_network(states).gather(1, actions)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.mse_loss(predicted_q_values, target_q_values)\n",
        "\n",
        "    # Backpropagate loss and update weights\n",
        "\n",
        "    self.optimizer.zero_grad() # Init optimzer\n",
        "    loss.backward() # Back propagate\n",
        "    nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # Gradient clipping\n",
        "    self.optimizer.step() # Update weights\n",
        "\n",
        "    # Soft update does not improve Pac-Man results"
      ],
      "metadata": {
        "id": "qm9KQXdEOFMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train an Agent"
      ],
      "metadata": {
        "id": "FbYepYcmY7OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialze an agent and frame stacking\n",
        "agent = Agent(action_size=pac_man_action_size)"
      ],
      "metadata": {
        "id": "zZJ7qOczY9AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize training hyperparameters\n",
        "\n",
        "num_episodes = 2000\n",
        "max_time_steps_per_episode = 10000 # Need many time steps for Pac-Man\n",
        "\n",
        "# Epsilon greedy hyperparameters\n",
        "epsilon_start = 1.0\n",
        "epsilon_decay = 0.995 # Decay epsilon slowly\n",
        "epsilon_min = 0.01\n",
        "\n",
        "epsilon = epsilon_start\n",
        "\n",
        "# Window of scores on 100 episodes\n",
        "window_of_scores = deque(maxlen=100)"
      ],
      "metadata": {
        "id": "R8VL9xD6ZnPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_state = torch.zeros((64, 3, 128, 128))  # Batch size 64, 3 channels\n",
        "output = agent.q_network(dummy_state)\n",
        "print(output.shape)  # Check if the network processes the input correctly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9XBgcs13qkE",
        "outputId": "72a6b539-9587-45d4-f985-7230184c66b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training loop\n",
        "\n",
        "for episode in range(1, num_episodes+1):\n",
        "  # Reset environment to initial state\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "\n",
        "  # Agent learning\n",
        "\n",
        "  for t in range(max_time_steps_per_episode):\n",
        "    action = agent.act(state=state, epsilon=epsilon)\n",
        "\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "    reward = (reward - np.mean(reward)) / (np.std(reward) + 1e-5) # Normalize reward to stabilize training\n",
        "\n",
        "\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "    state = next_state\n",
        "    score += reward\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  window_of_scores.append(score)\n",
        "  epsilon = max(epsilon_min, epsilon_decay * epsilon) # Decay epsilon\n",
        "\n",
        "  # Print stuff to get feedback that agent is working\n",
        "  print(f\"\\rEpisode: {episode}\\tScore: {score}\\tAverage Score: {np.mean(window_of_scores)}\", end=\"\") # \\r allows newly printed line to over-ride previous one\n",
        "\n",
        "  if episode % 100 == 0:\n",
        "    print(\"\")\n",
        "\n",
        "  if np.mean(window_of_scores) >= 500: # Score is score on Pac-Man\n",
        "    print(f\"\\nEnvironment solved in {episode} episodes!\\t Average Score: {np.mean(window_of_scores)}\")\n",
        "\n",
        "    torch.save(agent.q_network.state_dict(), \"pac_man_model.pth\") # Save parameters to PyTorch file\n",
        "\n",
        "    break # No more training needed\n"
      ],
      "metadata": {
        "id": "otF3obiYa-ha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "6d546db1-2db7-4ed8-8f64-ebe46e51e7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-bd15af3d8a6e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-3fa2bc0ad7de>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-3fa2bc0ad7de>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Back propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Soft update does not improve Pac-Man results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "arP7nNDooonT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "O2kgCLeIpZ3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers import RecordVideo"
      ],
      "metadata": {
        "id": "xicp0-kWpbQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video"
      ],
      "metadata": {
        "id": "SM0mHRMLLepJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state) # AI is in inference mode after training is done\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v4')\n",
        "\n",
        "def show_video():\n",
        "    # Show video in notebook\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "aRBuWsBio9kA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}