{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpx0ZfkVA7_D"
      },
      "source": [
        "# Lunar Landing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwtJYbZB-sx"
      },
      "source": [
        "Traning double dueling deep Q-learning AI to solve [lunar landing environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/) from [Gymnasium](https://gymnasium.farama.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oit0CcXdBBdS"
      },
      "source": [
        "## Intalling packages and importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRU22eeByqt"
      },
      "source": [
        "### Installing NumPy and PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IVlP4lYB7wv"
      },
      "outputs": [],
      "source": [
        "%pip install numpy\n",
        "%pip install torch\n",
        "%pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8bunpN1BGPW"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIk8QauYBIFx",
        "outputId": "b957d089-4396-4041-e7f2-04041b5e6118"
      },
      "outputs": [],
      "source": [
        "%pip install gymnasium\n",
        "%pip install swig # Necessary to build the wheel for box2d-py\n",
        "%pip install gymnasium[box2d] # Contains lunar lander environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvBDak4PBPVm"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oL8gHM8LBS6p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Pytorch stuff\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # Neural network library\n",
        "import torch.optim as optim # Optimizer to train AI\n",
        "import torch.nn.functional as F # Activation function\n",
        "import torch.autograd as autograd # Stochastic gradient descent for neural net trainig\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0zU9DmTB7Bw"
      },
      "source": [
        "## Building AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvYHltR6EP9P"
      },
      "source": [
        "### Neural Net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Om190UxbCVM2"
      },
      "outputs": [],
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Dueling Deep Q-Network\n",
        "\n",
        "    Dueling separates the value of the state from the value of the actions possible in that state\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Shared fully connected layers\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "\n",
        "        # Value stream\n",
        "        self.value_fc = nn.Linear(128, 64)\n",
        "        self.value_out = nn.Linear(64, 1)\n",
        "\n",
        "        # Advantage stream\n",
        "        self.advantage_fc = nn.Linear(128, 64)\n",
        "        self.advantage_out = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # State value stream\n",
        "        value = F.relu(self.value_fc(x))\n",
        "        value = self.value_out(value)\n",
        "\n",
        "        # Advantage stream\n",
        "        advantage = F.relu(self.advantage_fc(x))\n",
        "        advantage = self.advantage_out(advantage)\n",
        "\n",
        "        # Combine value and advantage\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO-0WutWHxSt"
      },
      "source": [
        "## Training AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVppFmoNIApl"
      },
      "source": [
        "### Set up Lunar Lander environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tLOSV7OKH4RH"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "\n",
        "lunar_state_size = env.observation_space.shape[0]\n",
        "lunar_action_size = env.action_space.n\n",
        "\n",
        "# Check that environment is set up correctly\n",
        "# Values are 8 and 4 as of lunar lander v3\n",
        "assert(lunar_state_size == 8)\n",
        "assert(lunar_action_size == 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdlsp4DJ_8Y"
      },
      "source": [
        "### Initialize hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BlzrduKhJlr0"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-4 # Optimized for lunar landing\n",
        "mini_batch_size = 100 # Standard for deep Q learning\n",
        "discount_factor = 0.99 # Optimal discount factor\n",
        "replay_buffer_size = int(1e5) # Number of experiences stored in memory (1 million is too slow on Colab but would provide better results per episode)\n",
        "tau = 1e-3 # Optimal value for tau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ayxD3EJLF8U"
      },
      "source": [
        "### Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D9q8yXHbLHe4"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "  def __init__(self, capacity) -> None:\n",
        "    # Capacity is capacity of memory\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration if possible\n",
        "    self.capacity = capacity\n",
        "    self.memory = deque(maxlen=capacity)\n",
        "\n",
        "  def push(self, event):\n",
        "    # Add experiences to replay memory buffer\n",
        "    self.memory.append(event)\n",
        "\n",
        "    # Make sure memory buffer capacity is not exceeded\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0] # Delete oldest memory\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    \"\"\"\n",
        "    Randomly sample experiences from memory\n",
        "    \"\"\"\n",
        "\n",
        "    experiences = random.sample(self.memory, batch_size)\n",
        "\n",
        "    # Convert elements of experience to PyTorch tensors and move them to device\n",
        "\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device) # Actions are either 0, 1, 2, 3\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "\n",
        "    # Convert Boolean data to float tensor\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device) # Last elements in experiences\n",
        "\n",
        "\n",
        "    return (states, next_states, actions, rewards, dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c-mCCBKOCHu"
      },
      "source": [
        "### Double Deep Q Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qm9KQXdEOFMt"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "  \"\"\"\n",
        "  Use double deep Q-learning with gradient clipping to reduce overestimation bias\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, state_size, action_size) -> None:\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration if possible\n",
        "\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "\n",
        "    # Q learning\n",
        "    self.q_network = DuelingDQN(state_size, action_size).to(self.device) # Local Q network\n",
        "    self.target_network = DuelingDQN(state_size, action_size).to(self.device) # Target Q network\n",
        "\n",
        "    self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate) # Optimizer for Q network\n",
        "\n",
        "    self.memory = ReplayMemory(replay_buffer_size)\n",
        "\n",
        "    self.time_step = 0 # Time step for updating target network\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done)) # Add experience to replay memory\n",
        "\n",
        "    self.time_step = (self.time_step + 1) % 4 # Learn every 4 steps\n",
        "\n",
        "    # Learn every 4 steps\n",
        "    if self.time_step == 0 and len(self.memory.memory) > mini_batch_size:\n",
        "      experiences = self.memory.sample(mini_batch_size)\n",
        "      self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon=0):\n",
        "    \"\"\"\n",
        "    Select action based on given state in environment using epsilon-greedy action selection policy.\n",
        "\n",
        "    Epsilon-greedy is standard in Deep Q learning over softmax. It is also simpler and less computationally expensive.\n",
        "    \"\"\"\n",
        "\n",
        "    # Important to add dimension that includes which batch the state belongs to\n",
        "    # First dimension of new state tensor is batch number\n",
        "\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) # Convert state to tensor and add batch\n",
        "\n",
        "    self.q_network.eval()\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation (make sure in inference mode)\n",
        "      action_values = self.q_network(state)\n",
        "    self.q_network.train()\n",
        "\n",
        "    # Use epsilon greedy action-selection policy\n",
        "\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    \"\"\"\n",
        "    Update Q-values based on sampled experiences\n",
        "    \"\"\"\n",
        "\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "\n",
        "    # Get best actions from the local Q-network\n",
        "    next_actions = self.q_network(next_states).detach().argmax(1).unsqueeze(1)\n",
        "\n",
        "    # Get corresponding Q-values from the target Q-network\n",
        "    next_q_targets = self.target_network(next_states).gather(1, next_actions)\n",
        "\n",
        "    # Compute target Q-values\n",
        "    target_q_values = rewards + (gamma * next_q_targets * (1 - dones))\n",
        "\n",
        "    # Compute current Q-values\n",
        "    predicted_q_values = self.q_network(states).gather(1, actions)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(predicted_q_values, target_q_values)\n",
        "\n",
        "    # Optimize the model\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # Gradient clipping\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # Soft update target network\n",
        "    self.soft_update(self.q_network, self.target_network, tau)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    \"\"\"\n",
        "    Update target network parameters based on weighted average of local network and target network parameters\n",
        "\n",
        "    Soft update prevents abrupt changes in target network parameters that could destabilize the training\n",
        "    \"\"\"\n",
        "\n",
        "    for local_param, target_param in zip(local_model.parameters(), target_model.parameters()):\n",
        "      target_param.data.copy_(tau * local_param.data + (1.0-tau) * target_param.data) # Soft update formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbYepYcmY7OF"
      },
      "source": [
        "### Train an Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zZJ7qOczY9AH"
      },
      "outputs": [],
      "source": [
        "# Initialze an agent\n",
        "agent = Agent(state_size=lunar_state_size, action_size=lunar_action_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R8VL9xD6ZnPp"
      },
      "outputs": [],
      "source": [
        "# Initialize training hyperparameters\n",
        "\n",
        "num_episodes = 2000\n",
        "max_time_steps_per_episode = 1000\n",
        "\n",
        "# Epsilon greedy hyperparameters\n",
        "epsilon_start = 1.0\n",
        "epsilon_decay = 0.995 # Decay epsilon slowly\n",
        "epsilon_min = 0.01\n",
        "\n",
        "epsilon = epsilon_start\n",
        "\n",
        "# Window of scores on 100 episodes\n",
        "window_of_scores = deque(maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otF3obiYa-ha",
        "outputId": "ba6f7d41-c5af-4ac6-bbe8-50dc0a6f7d9c"
      },
      "outputs": [],
      "source": [
        "# Final training loop\n",
        "\n",
        "for episode in range(1, num_episodes+1):\n",
        "  # Reset environment to initial state\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "\n",
        "  # Agent learning\n",
        "\n",
        "  for t in range(max_time_steps_per_episode):\n",
        "    action = agent.act(state=state, epsilon=epsilon)\n",
        "\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "    state = next_state\n",
        "    score += reward\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  window_of_scores.append(score)\n",
        "  epsilon = max(epsilon_min, epsilon_decay * epsilon) # Decay epsilon\n",
        "\n",
        "  # Print stuff to get feedback that agent is working\n",
        "  print(f\"\\rEpisode: {episode}\\tScore: {score}\\tAverage Score: {np.mean(window_of_scores)}\", end=\"\") # \\r allows newly printed line to over-ride previous one\n",
        "\n",
        "  if episode % 100 == 0:\n",
        "    print(\"\")\n",
        "\n",
        "  if np.mean(window_of_scores) >= 200: # Succesful episodes have scores 200 or above, so model is successful on average\n",
        "    print(f\"\\nEnvironment solved in {episode} episodes!\\t Average Score: {np.mean(window_of_scores)}\")\n",
        "\n",
        "    torch.save(agent.q_network.state_dict(), \"lunar_landing_model.pth\") # Save parameters to PyTorch file\n",
        "\n",
        "    break # No more training needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arP7nNDooonT"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2kgCLeIpZ3x"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xicp0-kWpbQJ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6KgeB-MLRnX"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "aRBuWsBio9kA",
        "outputId": "28df1b6b-88cc-4ced-936a-e832198c57b0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state) # AI is in inference mode after training is done\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v3')\n",
        "\n",
        "def show_video():\n",
        "    # Show video in notebook\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
